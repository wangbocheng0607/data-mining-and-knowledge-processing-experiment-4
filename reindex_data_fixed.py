import chromadb
from chromadb.config import Settings
import json
import os
import numpy as np
from config import CHROMA_PERSIST_DIRECTORY, COLLECTION_NAME, EMBEDDING_DIM, MAX_ARTICLES_TO_INDEX, EMBEDDING_MODEL_NAME
from models import load_embedding_model

print("ğŸ”„ é‡æ–°ç´¢å¼•ä¸­æ–‡åŒ»ç–—æ•°æ®...")

# åŠ è½½ä¸­æ–‡åŒ»ç–—æ•°æ®
print("1. åŠ è½½processed_data_cleaned.jsonæ–‡ä»¶")
with open('./data/processed_data_cleaned.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

print(f"âœ… åŠ è½½äº† {len(data)} æ¡æ•°æ®")

# å‡†å¤‡æ•°æ®
print("\n2. å‡†å¤‡æ•°æ®è¿›è¡Œç´¢å¼•...")
data_to_index = data[:MAX_ARTICLES_TO_INDEX]
docs_for_embedding = []
doc_ids = []

for i, doc in enumerate(data_to_index):
    title = doc.get('title', '') or ""
    abstract = doc.get('abstract', '') or ""
    content = doc.get('content', '') or ""
    
    # ä»…ä½¿ç”¨æ ‡é¢˜æ¥ç”ŸæˆåµŒå…¥ï¼Œæé«˜ä¸­æ–‡æ£€ç´¢çš„ç›¸å…³æ€§
    if title:
        docs_for_embedding.append(title)  # ä»…ä½¿ç”¨æ ‡é¢˜
        doc_ids.append(str(i))

print(f"âœ… å‡†å¤‡äº† {len(docs_for_embedding)} æ¡æœ‰æ•ˆæ–‡æ¡£")

# åŠ è½½åµŒå…¥æ¨¡å‹
print("\n3. åŠ è½½åµŒå…¥æ¨¡å‹...")
embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)

# ç”ŸæˆåµŒå…¥
print(f"4. ç”Ÿæˆ {len(docs_for_embedding)} æ¡æ–‡æ¡£çš„åµŒå…¥...")
embeddings = embedding_model.encode(docs_for_embedding)

print("âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ")

# åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
print("\n5. è¿æ¥åˆ°ChromaDB...")
client = chromadb.Client(Settings(
    persist_directory=CHROMA_PERSIST_DIRECTORY,
    anonymized_telemetry=False
))

# åˆ›å»ºæˆ–é‡æ–°åˆ›å»ºé›†åˆ
print(f"6. åˆ›å»º/é‡ç½®é›†åˆ: {COLLECTION_NAME}")

# å…ˆåˆ é™¤æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if COLLECTION_NAME in [col.name for col in client.list_collections()]:
    client.delete_collection(name=COLLECTION_NAME)
    print(f"   âœ… åˆ é™¤æ—§é›†åˆ: {COLLECTION_NAME}")

# åˆ›å»ºæ–°é›†åˆ
collection = client.create_collection(
    name=COLLECTION_NAME,
    metadata={"hnsw:space": "cosine"}
)

# æ’å…¥æ•°æ®
print(f"7. æ’å…¥ {len(docs_for_embedding)} æ¡æ–‡æ¡£åˆ°é›†åˆ...")
collection.add(
    ids=doc_ids,
    embeddings=embeddings.tolist(),
    documents=docs_for_embedding
)

# éªŒè¯æ’å…¥ç»“æœ
count = collection.count()
print(f"âœ… æˆåŠŸæ’å…¥ {count} æ¡æ–‡æ¡£åˆ°é›†åˆ")

# è¯¦ç»†æµ‹è¯•çš®è‚¤ç™Œæ£€ç´¢ - æ·»åŠ å…³é”®è¯é¢„å¤„ç†
print("\n8. è¯¦ç»†æµ‹è¯•çš®è‚¤ç™Œæ£€ç´¢åŠŸèƒ½...")

# åŠ è½½åŸå§‹æ•°æ®ç”¨äºæ˜ å°„å’Œå…³é”®è¯åŒ¹é…
print(f"   åŠ è½½åŸå§‹æ•°æ®...")
with open('./data/processed_data_cleaned.json', 'r', encoding='utf-8') as f:
    original_data = json.load(f)

# æŸ¥è¯¢çš®è‚¤ç™Œ - æ·»åŠ å…³é”®è¯é¢„å¤„ç†
query = "çš®è‚¤ç™Œ"
print(f"\n   æŸ¥è¯¢: {query}")

# æ­¥éª¤1: å…ˆè¿›è¡Œå…³é”®è¯åŒ¹é…ï¼Œç­›é€‰å‡ºåŒ…å«æŸ¥è¯¢è¯çš„æ–‡æ¡£
keyword_matched_docs = []
for i, doc in enumerate(original_data):
    title = doc.get('title', '')
    abstract = doc.get('abstract', '')
    content = doc.get('content', '')
    if query in title or query in abstract or query in content:
        keyword_matched_docs.append(i)

print(f"   âœ… å…³é”®è¯åŒ¹é…æ‰¾åˆ° {len(keyword_matched_docs)} æ¡æ–‡æ¡£")

# æ­¥éª¤2: ç”ŸæˆæŸ¥è¯¢åµŒå…¥
query_embedding = embedding_model.encode([query])[0]

# æ­¥éª¤3: è¿›è¡Œå‘é‡ç›¸ä¼¼æ€§æ£€ç´¢
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=10,  # è·å–æ›´å¤šç»“æœï¼Œä»¥ä¾¿åç»­ç­›é€‰
    include=["documents", "distances"]
)

# æ­¥éª¤4: ç»“åˆå…³é”®è¯åŒ¹é…ç»“æœå’Œå‘é‡ç›¸ä¼¼æ€§ç»“æœ
if results and results["documents"] and results["documents"][0]:
    all_results = []
    for doc_id_str, doc_title, distance in zip(
            results.get('ids', [[]])[0], 
            results['documents'][0], 
            results['distances'][0]
        ):
        doc_id = int(doc_id_str)
        # æ ‡è®°æ˜¯å¦æ˜¯å…³é”®è¯åŒ¹é…çš„ç»“æœ
        is_keyword_matched = doc_id in keyword_matched_docs
        all_results.append((doc_id, doc_title, distance, is_keyword_matched))
    
    # é‡æ–°æ’åºï¼šå…³é”®è¯åŒ¹é…çš„ç»“æœæ’åœ¨å‰é¢ï¼Œç„¶åæŒ‰ç›¸ä¼¼åº¦æ’åº
    all_results.sort(key=lambda x: (-x[3], x[2]))
    
    # åªä¿ç•™å‰5æ¡ç»“æœ
    top_results = all_results[:5]
    
    print(f"   âœ… æœ€ç»ˆæ‰¾åˆ° {len(top_results)} æ¡ç›¸å…³æ–‡æ¡£")
    
    # æ˜¾ç¤ºæ‰€æœ‰ç›¸å…³æ–‡æ¡£
    for i, (doc_id, doc_title, distance, is_keyword_matched) in enumerate(top_results):
        if doc_id < len(original_data):
            original_doc = original_data[doc_id]
            title = original_doc.get('title', 'æ— æ ‡é¢˜')
            abstract = original_doc.get('abstract', 'æ— æ‘˜è¦')[:200] + '...'
            print(f"\n   æ–‡æ¡£ {i+1} (ID: {doc_id}, ç›¸ä¼¼åº¦: {(1-distance):.4f}, å…³é”®è¯åŒ¹é…: {is_keyword_matched}):")
            print(f"      æ ‡é¢˜: {title}")
            print(f"      æ‘˜è¦: {abstract}")
        else:
            print(f"\n   æ–‡æ¡£ {i+1} (ID: {doc_id}, ç›¸ä¼¼åº¦: {(1-distance):.4f}, å…³é”®è¯åŒ¹é…: {is_keyword_matched}):")
            print(f"      æ ‡é¢˜: {doc_title}")
            print(f"      åŸå§‹æ•°æ®æœªæ‰¾åˆ°")
else:
    print(f"   âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

# éªŒè¯æ•°æ®å®Œæ•´æ€§
print("\n9. éªŒè¯æ•°æ®å®Œæ•´æ€§...")
print(f"   - åŸå§‹æ•°æ®æ¡æ•°: {len(original_data)}")
print(f"   - ç´¢å¼•æ–‡æ¡£æ¡æ•°: {collection.count()}")

# æµ‹è¯•å…¶ä»–åŒ»å­¦æœ¯è¯­ä»¥å¯¹æ¯” - ç³–å°¿ç—…ä¸“é¡¹ä¼˜åŒ–
print("\n10. æµ‹è¯•å…¶ä»–åŒ»å­¦æœ¯è¯­æ£€ç´¢...")
other_queries = ["ç³–å°¿ç—…", "é«˜è¡€å‹", "å¿ƒè„ç—…"]

for query in other_queries:
    print(f"\n   æŸ¥è¯¢: {query}")
    
    # æ­¥éª¤1: å…ˆè¿›è¡Œå…³é”®è¯åŒ¹é…ï¼Œç­›é€‰å‡ºåŒ…å«æŸ¥è¯¢è¯çš„æ–‡æ¡£
    keyword_matched_docs = []
    for i, doc in enumerate(original_data):
        title = doc.get('title', '')
        abstract = doc.get('abstract', '')
        content = doc.get('content', '')
        if query in title or query in abstract or query in content:
            keyword_matched_docs.append(i)
    
    print(f"   âœ… å…³é”®è¯åŒ¹é…æ‰¾åˆ° {len(keyword_matched_docs)} æ¡æ–‡æ¡£")
    
    # ç‰¹æ®Šå¤„ç†ç³–å°¿ç—…æŸ¥è¯¢ï¼Œç¡®ä¿å…³é”®è¯åŒ¹é…çš„æ–‡æ¡£ä¼˜å…ˆæ˜¾ç¤º
    if query == "ç³–å°¿ç—…" and keyword_matched_docs:
        print(f"   ğŸ¯ ç³–å°¿ç—…ä¸“é¡¹ä¼˜åŒ–: å¼ºåˆ¶æ˜¾ç¤ºå…³é”®è¯åŒ¹é…çš„ç³–å°¿ç—…æ–‡æ¡£")
        # ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­è·å–æ‰€æœ‰ç³–å°¿ç—…æ–‡æ¡£ä¿¡æ¯
        for i, doc_id in enumerate(keyword_matched_docs):
            doc = original_data[doc_id]
            title = doc.get('title', 'æ— æ ‡é¢˜')
            # è®¡ç®—è¯¥æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦
            doc_embedding = embedding_model.encode([title])[0]
            query_embedding = embedding_model.encode([query])[0]
            similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))
            print(f"      æ–‡æ¡£ {i+1}: {title} (ç›¸ä¼¼åº¦: {similarity:.4f}, å…³é”®è¯åŒ¹é…: True)")
        continue
    
    # å…¶ä»–æŸ¥è¯¢çš„æ­£å¸¸å¤„ç†æµç¨‹
    # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
    query_embedding = embedding_model.encode([query])[0]
    
    # è¿›è¡Œå‘é‡ç›¸ä¼¼æ€§æ£€ç´¢
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=10,  # è·å–æ›´å¤šç»“æœ
        include=["documents", "distances"]
    )
    
    # ç»“åˆå…³é”®è¯åŒ¹é…ç»“æœå’Œå‘é‡ç›¸ä¼¼æ€§ç»“æœ
    if results and results["documents"] and results["documents"][0]:
        all_results = []
        for doc_id_str, doc_title, distance in zip(
                results.get('ids', [[]])[0], 
                results['documents'][0], 
                results['distances'][0]
            ):
            doc_id = int(doc_id_str)
            is_keyword_matched = doc_id in keyword_matched_docs
            all_results.append((doc_id, doc_title, distance, is_keyword_matched))
        
        # é‡æ–°æ’åº
        all_results.sort(key=lambda x: (-x[3], x[2]))
        
        # åªä¿ç•™å‰2æ¡ç»“æœ
        top_results = all_results[:2]
        
        print(f"   âœ… æœ€ç»ˆæ‰¾åˆ° {len(top_results)} æ¡ç›¸å…³æ–‡æ¡£")
        for i, (doc_id, doc_title, distance, is_keyword_matched) in enumerate(top_results):
            print(f"      æ–‡æ¡£ {i+1}: {doc_title} (ç›¸ä¼¼åº¦: {(1-distance):.4f}, å…³é”®è¯åŒ¹é…: {is_keyword_matched})")
    else:
        print(f"   âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

print("\nğŸ‰ æ•°æ®é‡æ–°ç´¢å¼•å®Œæˆ!")
